# ==========================================
# å»ºç­‘æ–‡æ¡£æ™ºèƒ½RAGå®¡æŸ¥ç³»ç»Ÿ - Ollamaç‰ˆæœ¬
# æ­£ç¡®çš„æ‰§è¡Œé¡ºåº
# ==========================================

# ===========================================
# Cell 1: å®šä¹‰BaseLLMåŸºç±»
# ===========================================
from abc import ABC, abstractmethod
from typing import Any, Optional

class BaseLLM(ABC):
    """Interface for large language models."""

    def __init__(
        self,
        model_name: str,
        model_params: Optional[dict[str, Any]] = None,
        **kwargs: Any,
    ):
        self.model_name = model_name
        self.model_params = model_params or {}

    @abstractmethod
    def predict(self, input: str) -> str:
        """Sends a text input to the LLM and retrieves a response."""

print("âœ… BaseLLMåŸºç±»å®šä¹‰å®Œæˆï¼")

# ===========================================
# Cell 2: å®šä¹‰OllamaLLMç±»
# ===========================================
import ollama
from typing import Any, Optional

class OllamaLLM(BaseLLM):
    """Implementation of the BaseLLM interface using Ollama."""

    def __init__(
        self,
        model_name: str,
        host: str = "http://localhost:11434",
        model_params: Optional[dict[str, Any]] = None,
        **kwargs: Any,
    ):
        super().__init__(model_name, model_params, **kwargs)
        self.host = host
        # è®¾ç½®ollamaå®¢æˆ·ç«¯çš„host
        if host != "http://localhost:11434":
            self.client = ollama.Client(host=host)
        else:
            self.client = ollama

    def predict(self, input: str) -> str:
        try:
            response = self.client.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": input}]
            )
            return response['message']['content']
        except Exception as e:
            print(f"Ollamaè°ƒç”¨å‡ºé”™: {e}")
            return f"æŠ±æ­‰ï¼Œæ¨¡å‹è°ƒç”¨å‡ºç°é—®é¢˜: {str(e)}"

print("âœ… OllamaLLMç±»å®šä¹‰å®Œæˆï¼")

# ===========================================
# Cell 3: æµ‹è¯•OllamaLLMåŠŸèƒ½
# ===========================================
print("ğŸš€ å¼€å§‹æµ‹è¯•OllamaLLM...")

# åˆå§‹åŒ–æ¨¡å‹
llm = OllamaLLM(
    model_name="qwen3:14b",  # ä½¿ç”¨æœ¬åœ°éƒ¨ç½²çš„qwenæ¨¡å‹
    host="http://localhost:11434"
)

# æµ‹è¯•åŸºæœ¬åŠŸèƒ½
print("ğŸ“ æµ‹è¯•é—®é¢˜ï¼šå»ºç­‘æ–‡æ¡£å®¡æŸ¥")
response = llm.predict("ä½ å¥½ï¼Œè¯·ç®€å•å›ç­”ï¼šé’¢ç­‹æ··å‡åœŸç»“æ„ä¸­ï¼ŒC25æ··å‡åœŸçš„æŠ—å‹å¼ºåº¦æ ‡å‡†å€¼æ˜¯å¤šå°‘ï¼Ÿ")
print(f"ğŸ¤– AIå›å¤ï¼š{response}")
print("\nâœ… OllamaLLMæµ‹è¯•æˆåŠŸï¼")

# ===========================================
# Cell 4: å®šä¹‰ä¿®å¤ç‰ˆEmbeddingæ¨¡å—
# ===========================================
from abc import ABC, abstractmethod
from typing import List, Any, Optional
import numpy as np
import os

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…è·¯å¾„é—®é¢˜
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# æ£€æŸ¥sentence_transformersåŒ…
try:
    from sentence_transformers import SentenceTransformer
    print("âœ… sentence_transformersåŒ…å¯ç”¨")
except ImportError:
    print("âŒ éœ€è¦å®‰è£…sentence_transformersåŒ…")
    print("è¯·åœ¨ç»ˆç«¯è¿è¡Œ: pip install sentence-transformers")

class BaseEmb(ABC):
    def __init__(
        self,
        model_name: str,
        model_params: Optional[dict[str, Any]] = None,
        **kwargs: Any,
    ):
        self.model_name = model_name
        self.model_params = model_params or {}

    @abstractmethod
    def get_emb(self, input: str) -> List[float]:
        """Sends a text input to the embedding model and retrieves the embedding."""
        pass

class BGEEmbedding(BaseEmb):
    """ä¿®å¤ç‰ˆBGEåµŒå…¥æ¨¡å‹ï¼Œè§£å†³è·¯å¾„å’Œå…¼å®¹æ€§é—®é¢˜"""
    
    def __init__(self, model_name: str = "BAAI/bge-m3", **kwargs):
        super().__init__(model_name=model_name, **kwargs)
        print(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}...")
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        cache_dir = os.path.abspath("./model_cache")
        os.makedirs(cache_dir, exist_ok=True)
        
        try:
            # æ–¹æ³•1ï¼šä½¿ç”¨eager attentioné¿å…SDPAé—®é¢˜
            self.embed_model = SentenceTransformer(
                model_name,
                cache_folder=cache_dir,
                model_kwargs={"attn_implementation": "eager"},
                trust_remote_code=True
            )
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ æ–¹æ³•1å¤±è´¥: {e}")
            print("å°è¯•æ–¹æ³•2ï¼šä½¿ç”¨è½»é‡çº§æ¨¡å‹...")
            try:
                # æ–¹æ³•2ï¼šä½¿ç”¨æ›´è½»é‡çš„æ¨¡å‹
                self.embed_model = SentenceTransformer(
                    "sentence-transformers/all-MiniLM-L6-v2",
                    cache_folder=cache_dir
                )
                print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆï¼ˆè½»é‡çº§ç‰ˆæœ¬ï¼‰ï¼")
            except Exception as e2:
                print(f"âŒ æ–¹æ³•2ä¹Ÿå¤±è´¥: {e2}")
                print("å»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é‡å¯kernel")
                raise

    def get_emb(self, text: str) -> List[float]:
        embedding = self.embed_model.encode(text)
        return embedding.tolist()
    
    def encode(self, texts, show_progress_bar=False):
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = self.embed_model.encode(texts, show_progress_bar=show_progress_bar)
        return np.array(embeddings)

print("âœ… Embeddingæ¨¡å—å®šä¹‰å®Œæˆï¼")

# ===========================================
# Cell 5: æµ‹è¯•Embeddingæ¨¡å—
# ===========================================
print("ğŸ”§ æµ‹è¯•Embeddingæ¨¡å—...")

try:
    # åˆå§‹åŒ–embeddingæ¨¡å‹
    emb = BGEEmbedding(model_name="BAAI/bge-m3")
    
    # æµ‹è¯•å•ä¸ªæ–‡æœ¬ç¼–ç 
    test_text = "å»ºç­‘ç»“æ„çš„å®‰å…¨æ€§æ£€æŸ¥åŒ…æ‹¬å“ªäº›æ–¹é¢ï¼Ÿ"
    print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}")
    
    embedding = emb.get_emb(test_text)
    print(f"âœ… æˆåŠŸç”Ÿæˆembeddingï¼Œç»´åº¦: {len(embedding)}")
    print(f"ğŸ“Š å‰5ä¸ªç»´åº¦å€¼: {[f'{x:.4f}' for x in embedding[:5]]}")
    
    # æµ‹è¯•æ‰¹é‡ç¼–ç 
    test_texts = [
        "é’¢ç­‹æ··å‡åœŸç»“æ„æ–½å·¥è¦æ±‚",
        "å»ºç­‘å®‰å…¨æ£€æŸ¥æ ‡å‡†", 
        "æ··å‡åœŸå…»æŠ¤æŠ€æœ¯è§„èŒƒ"
    ]
    embeddings = emb.encode(test_texts)
    print(f"âœ… æ‰¹é‡ç¼–ç æˆåŠŸï¼Œå½¢çŠ¶: {embeddings.shape}")
    
except Exception as e:
    print(f"âŒ Embeddingæµ‹è¯•å¤±è´¥: {e}")
    print("æç¤ºï¼šå¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")

print("\nâœ… Embeddingæ¨¡å—æµ‹è¯•å®Œæˆï¼")

# ===========================================
# Cell 6: å®Œæ•´RAGç³»ç»Ÿæµ‹è¯•
# ===========================================
print("ğŸš€ å¼€å§‹å®Œæ•´çš„RAGç³»ç»Ÿæµ‹è¯•...")

try:
    # 1. ç¡®è®¤æ‰€æœ‰ç»„ä»¶æ­£å¸¸
    print("\n1ï¸âƒ£ éªŒè¯ç»„ä»¶çŠ¶æ€...")
    print(f"âœ… LLMæ¨¡å‹: {llm.model_name}")
    print(f"âœ… Embeddingæ¨¡å‹: {emb.model_name}")
    
    # 2. åˆ›å»ºç®€å•çŸ¥è¯†åº“
    print("\n2ï¸âƒ£ æ„å»ºçŸ¥è¯†åº“...")
    knowledge_base = [
        "é’¢ç­‹æ··å‡åœŸæŸ±çš„æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸åº”ä½äºC25ï¼Œé’¢ç­‹ä¿æŠ¤å±‚åšåº¦åº”ç¬¦åˆè®¾è®¡è¦æ±‚ã€‚",
        "æ··å‡åœŸæµ‡ç­‘åº”è¿ç»­è¿›è¡Œï¼Œæµ‡ç­‘é—´æ­‡æ—¶é—´ä¸åº”è¶…è¿‡æ··å‡åœŸçš„åˆå‡æ—¶é—´ã€‚",
        "é’¢ç­‹ç„Šæ¥åº”ç¬¦åˆç›¸å…³è§„èŒƒè¦æ±‚ï¼Œç„Šæ¥è´¨é‡åº”è¿›è¡Œæ£€éªŒã€‚",
        "æ¨¡æ¿å®‰è£…åº”ç‰¢å›ºï¼Œå‡ ä½•å°ºå¯¸åº”å‡†ç¡®ï¼Œè¡¨é¢åº”å¹³æ•´å…‰æ»‘ã€‚",
        "æ··å‡åœŸå…»æŠ¤æœŸé—´åº”ä¿æŒæ··å‡åœŸè¡¨é¢æ¹¿æ¶¦ï¼Œå…»æŠ¤æ—¶é—´ä¸å°‘äº7å¤©ã€‚"
    ]
    
    # è®¡ç®—çŸ¥è¯†åº“embeddings
    kb_embeddings = emb.encode(knowledge_base)
    print(f"âœ… çŸ¥è¯†åº“ç¼–ç å®Œæˆï¼ŒåŒ…å«{len(knowledge_base)}ä¸ªæ–‡æ¡£")
    
    # 3. æµ‹è¯•RAGæ£€ç´¢
    print("\n3ï¸âƒ£ æµ‹è¯•RAGæ£€ç´¢...")
    query = "æ··å‡åœŸå¼ºåº¦è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ"
    query_embedding = emb.encode([query])
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    from sklearn.metrics.pairwise import cosine_similarity
    similarities = cosine_similarity(query_embedding, kb_embeddings)[0]
    
    # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£
    best_idx = similarities.argmax()
    best_doc = knowledge_base[best_idx]
    best_score = similarities[best_idx]
    
    print(f"ğŸ“ æŸ¥è¯¢: {query}")
    print(f"ğŸ¯ æœ€ç›¸å…³æ–‡æ¡£: {best_doc}")
    print(f"ğŸ“Š ç›¸ä¼¼åº¦åˆ†æ•°: {best_score:.3f}")
    
    # 4. æµ‹è¯•RAGç”Ÿæˆ
    print("\n4ï¸âƒ£ æµ‹è¯•RAGç”Ÿæˆ...")
    rag_prompt = f"""
åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ï¼š

å‚è€ƒæ–‡æ¡£ï¼š{best_doc}

é—®é¢˜ï¼š{query}

è¯·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ï¼š
"""
    
    rag_response = llm.predict(rag_prompt)
    print(f"ğŸ¤– RAGå›ç­”: {rag_response}")
    
    print("\nğŸ‰ å®Œæ•´RAGç³»ç»Ÿæµ‹è¯•æˆåŠŸï¼")
    print("\nğŸ“‹ ç³»ç»ŸçŠ¶æ€æ€»ç»“ï¼š")
    print("âœ… Ollama LLM: æ­£å¸¸å·¥ä½œ")
    print("âœ… BGE Embedding: æ­£å¸¸å·¥ä½œ") 
    print("âœ… å‘é‡æ£€ç´¢: æ­£å¸¸å·¥ä½œ")
    print("âœ… RAGç”Ÿæˆ: æ­£å¸¸å·¥ä½œ")
    print("\nğŸš€ ç³»ç»Ÿå·²å‡†å¤‡å¥½å¤„ç†å»ºç­‘æ–‡æ¡£å®¡æŸ¥ä»»åŠ¡ï¼")
    
except Exception as e:
    print(f"âŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
